---
title: "Clustering"
author: "Mihail Krupin"
date: '03 Марта 2018 '
output:
  html_document:
    toc: yes
    toc_depth: 5
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '5'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(cluster)
require(ggplot2)
library(magrittr)
library(factoextra)
```

# Подготовка данных
Функция для объединения нескольких графиков
```{r }
multiplot <- function (..., plotlist = NULL, file, cols = 1, layout = NULL) {
  library(grid)
  plots <- c(list(...), plotlist)
  numPlots = length(plots)
  if (is.null(layout)) {
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)), 
      ncol = cols, nrow = ceiling(numPlots/cols))
  }
  if (numPlots == 1) {
    print(plots[[1]])
  }
  else {
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), 
      ncol(layout))))
    for (i in 1:numPlots) {
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row, 
        layout.pos.col = matchidx$col))
    }
  }
}

```

Удаляем наши не нужные столбцы `"Long"` и `"Lat"`. Первый признак - название города - делаем названием строки
```{r }

data <- read.table("Data.txt", header = TRUE)

data <- data[ -c(11:13)]
rownames(data) <- data[,1]
data[,1] <- NULL
```
Производим стандартизацию данных. Для признаков Housing и Crime - делаем так, чтобы они принимали обратные значения (для всех признаков, корме этих, больше=лучше)
```{r }
data <- scale(data)
for (i in data[,"HousingCost"]||data[,"Crime"]){
  i = -i;
}
```
# Кластеризация иерархическим методом
## Расстояния 
Перед кластеризацией иерархическим методом следует найти различные расстояни как между объектами, так и между кластерами. Потом мы сможем сравнить какое из растояний подходит лучше
```{r }
# Евклидово расстояние 
de <- dist(data)
           
# Манхэтанских кварталов
dm <- dist(data, method = "manhattan")

# Расстояние Чебышева
dcheb <- dist(data, method = "canberra")
```

Также напишем функцию, которая будет считать нам расстояния между кластерами. В качестве второго параметра этой функции мы будем принимать какое либо известное расстояние между кластерами
```{r }
foo <- function(d, method){
  x_hc <- hclust(d, method = method)
} 
```

Для того, чтоыб не занимать много времни на поиск подяходящих расстояний, возьме лишь 3 кластерных расстояний: `complete`, `average` и `centroid`. На этих расстояниях можно хорошо увидеть как относительно правильно выбранного расстояния изменяется монотонность

## Визуализация результатов для Евклидового расстояния
Для начала покажем дендрограммы всех кластерных расстояний на одном графике
```{r }
# Евклидово расстояние

ge1 <- foo(de, "complete")
ge2 <- foo(de, "average")
ge3 <- foo(de, "centroid")
```

```{r }
# Для остальных графиков функция такая же, изменяется только первый параметр

plot(ge1, hang = -1, cex = 0.001,
           main = "Complete Method")
```

```{r echo=FALSE}

plot(ge2, hang = -3, cex = 0.0001,
           main = "Average Method")
plot(ge3, hang = -1.5, cex = 0.001,
           main = "Centroid Method")

```

А также, для наглядности, построим график расстояний между кластерами, чтобы можно было легче понять, какое количество кластеров или какую высоту нужно брать для разбиения
```{r }
# Для остальных графиков функция такая же, изменяется только y

N <- nrow(data)
qplot(x = seq(N-1, 1, -1), 
      y = ge1$height, 
      geom = c("point", "line"),
      xlab = "K clusters",
      ylab = "Height",
      main = "Complete Method")
```

```{r echo=FALSE}
qplot(x = seq(N-1, 1, -1), 
      y = ge2$height, 
      geom = c("point", "line"),
      xlab = "K clusters",
      ylab = "Height",
      main = "Average Method")

qplot(x = seq(N-1, 1, -1), 
      y = ge3$height, 
      geom = c("point", "line"),
      xlab = "K clusters",
      ylab = "Height",
      main = "Centroid Method")

```


Теперь нужно сделать разбиение для каждого и найденных расстояний, чтобы посмотреть на сколько хорошо разделились наши объекты на кластеры
```{r }
cutting1e <- cutree(ge1, k=3)
cutting2e <- cutree(ge2, k=4) #4 culsters
cutting3e <- cutree(ge3, k=3) 

data_pca <- scale(data, 
                  scale = F) %>% 
  scale(., 
        center = F, 
        scale = apply(., 2, norm, type = "2")) %>% 
  prcomp %$% 
  x[ ,1:2] %>% 
  as.data.frame %>% 
  cbind(., 
        Clusters_3 = factor(cutting2e),
       Clusters_4 = factor(cutting3e))

multiplot(ggplot(data = data_pca, 
                 aes(x = '', 
                     y = cutting1e, 
                     color = cutting1e)) + 
            geom_jitter(color=cutting1e) + 
            ggtitle("Complete Method"),
          ggplot(data = data_pca, 
                 aes(x = '', 
                     y = cutting2e, 
                     color = cutting2e)) + 
            geom_jitter(color=cutting2e) + 
            ggtitle("Average Method"),
          ggplot(data = data_pca, 
                 aes(x = '', 
                     y = cutting3e, 
                     color = cutting3e)) + 
            geom_jitter(color=cutting3e) + 
            ggtitle("Centroid Method"),
          cols = 3)


```

В данном случае, при разбиение наших данных на кластеры с помощью выше визуализируемых графиков, можно определить какое количество кластеров или какую высоту нужно брать. 
<br> Важно заметить, что в `Centroid Method` нельзя выбрать высоту по дендрограмме, а можно выбрать лишь на какое количество кластеров мы будем разбивать данные. Такое случилось из за того, что при использование данного метода монотонность нашей дендрограммы нарушается, так как расстояния между кластерами не монотонны 


## Визуализация результатов для расстояния Манхэтанских кварталов
Для начала также покажем дендрограммы всех кластерных расстояний
```{r }
# Расстояние Манхэтанских кварталов

gm1 <- foo(dm, "complete")
gm2 <- foo(dm, "average")
gm3 <- foo(dm, "centroid")
```

```{r }
# Для остальных графиков функция такая же, изменяется только первый параметр

plot(gm1, hang = -1, cex = 0.001,
           main = "Complete Method")
```

```{r echo=FALSE}

plot(gm2, hang = -3, cex = 0.0001,
           main = "Average Method")
plot(gm3, hang = -1, cex = 0.01,
           main = "Centroid Method")

```

Также изобразим график расстояний между калстерами/объектами для наглядности
```{r }
# Для остальных графиков функция такая же, изменяется только y

N <- nrow(data)
qplot(x = seq(N-1, 1, -1), 
      y = gm1$height, 
      geom = c("point", "line"),
      xlab = "K clusters",
      ylab = "Height",
      main = "Complete Method")
```

```{r echo=FALSE}
qplot(x = seq(N-1, 1, -1), 
      y = gm2$height, 
      geom = c("point", "line"),
      xlab = "K clusters",
      ylab = "Height",
      main = "Average Method")

qplot(x = seq(N-1, 1, -1), 
      y = gm3$height, 
      geom = c("point", "line"),
      xlab = "K clusters",
      ylab = "Height",
      main = "Centroid Method")

```


А теперь сделаем разбиение наших объектов и визиализируем результат 
```{r }
cutting1m <- cutree(gm1, k=3)
cutting2m <- cutree(gm2, k=3) #4 culsters
cutting3m <- cutree(gm3, k=4) 

data_pca <- scale(data, 
                  scale = F) %>% 
  scale(., 
        center = F, 
        scale = apply(., 2, norm, type = "2")) %>% 
  prcomp %$% 
  x[ ,1:2] %>% 
  as.data.frame %>% 
  cbind(., 
        Clusters_3 = factor(cutting2m),
       Clusters_4 = factor(cutting3m))

multiplot(ggplot(data = data_pca, 
                 aes(x = '', 
                     y = cutting1m, 
                     color = cutting1m)) + 
            geom_jitter(color= cutting1m) +
            ggtitle("Complete Method"),
          ggplot(data = data_pca, 
                 aes(x = '', 
                     y = cutting2m, 
                     color = cutting2m)) + 
            geom_jitter(color=cutting2m) + 
            ggtitle("Average Method"),
          ggplot(data = data_pca, 
                 aes(x = '', 
                     y = cutting3m, 
                     color = cutting3m)) + 
            geom_jitter(color=cutting3m) + 
            ggtitle("Centroid Method"),
          cols = 3)

```


## Вывод

Несмотря на то, что были взяты не все известные расстояния, все равно можно сделать вывод, что если выбирать в качестве расстояния между объектам расстояние `Манхэтанских кварталов` и  в качестве расстояния между кластерами расстояние `Complete Method` можно добиться хорошего результата. При заданном количестве кластеров (7 кластеров) получается, по моему, хороший результат
<br> Многие из расстояний между кластеров плохо работают, именно поэтому при дальнейшем разбиение многие объекты относятся к первому кластеру. По моему мнени, а также исходя из дендрограмм, можно увидеть, что лучший метод - `Complete Method`
<br> Если же увеличивать количество кластеров (или уменьшать расстояние меджду кластерами) в других методах, то результат особо не увеличится


# Кластеризация с помощью алгоритма kmeans

Основная сложность в данном алгоритме заключается в том, что нужно определить оптимальные значения наших центров. Чтобы компенсировать этот нюанс мы можем запускать наш алгоритм несколько раз и из полученных результатов выбрать наилучший. Параметр `nstart` в функции `kmeans()` отвечает именно за это   

```{r }
data <- na.omit(data)

clust_data <- kmeans(data,
                     centers = 3,
                     nstart = 50)

```

Теперь же провизуализируем полученный результат 
```{r }
data_pca <- scale(data, 
                  scale = F) %>% 
  scale(., 
        center = F, 
        scale = apply(., 2, norm, type = "2")) %>% 
  prcomp %$% 
  x[ ,1:2] %>% 
  as.data.frame %>% 
  cbind(., 
        Clusters = factor(clust_data$cluster))

ggplot(data = data_pca, 
                 aes(x = '', 
                     y = clust_data$cluster, 
                     color = clust_data$cluster)) + 
            geom_jitter(color=clust_data$cluster) + 
            ggtitle("Kmeans")
```

## Оптимальное количество центров

Одна из важнейших задач во время применения алгоритма kmeans() - выбор правильного количества центров в пространстве. Для определения качества алгоритма вводится некий функционал качества, в качесвте которого выступает сумма расстояний между объектами. Для правильного подбора числа кластеров можно использовать метод каменистой осыпи, который показывает зависимость целевой переменной от количества кластеров 
```{r }
# С помощью этой формулы мы сможем рассчитать качество нашей кластреизаци  при различном количестве центров
D <- 2:30 %>% 
  sapply(function(x){cl <- kmeans(x = data, 
                                  centers = x, 
                                  iter.max = 100, 
                                  nstart = 100,
                                  algorithm = c("Hartigan-Wong")) 
                     cl$tot.withinss})

qplot(x = seq(2:30), 
      y = D, 
      geom = c("point", "line"),
      xlab = "Number Of Clusters",
      ylab = "Sum of square distance within clusters")

```

В данном графике мы рассмотрели изменение функции качества(расстояния между объектами) при использовании изначально до 50 центров. Мы можем увидеть как как сильно уменьшается наше расстояние между объектами при увеличении количества центров. Нужно заметить, что наименьшее значение функции качества не означае наилучший рещультат. Нужно выбрать оптимальное количество центров, например 7!

```{r }
clust_data_opt <- kmeans(data,
                     centers = 4,
                     nstart = 100)

data_pca <- scale(data, 
                  scale = F) %>% 
  scale(., 
        center = F, 
        scale = apply(., 2, norm, type = "2")) %>% 
  prcomp %$% 
  x[ ,1:2] %>% 
  as.data.frame %>% 
  cbind(., 
        Clusters = factor(clust_data_opt$cluster))


ggplot(data = data_pca, 
                 aes(x = '', 
                     y = clust_data_opt$cluster, 
                     color = clust_data_opt$cluster)) + 
            geom_jitter(color=clust_data_opt$cluster) + 
            ggtitle("Kmeans with optimized number of centers")
```

## Вывод 

Алгоритм `kmeans()` довольно хороший и быстродейсвтенный алгоритм. С помощью метода каменистого спуска мы смогли посмотреть, что при увеличении количество кластеров, мы будем получать наименьшие расстояния между объектами. Но это не всегда хорошо, нужно подобрать такое число, которое будет оптимально разбивать наши данные на кластеры. Не зря данный метод (и многие другие) называют эвритсикой:)  


# Кластеризация с помощью алгоритма PAM

Кроме алгоритмов, рассмотренных выше, есть еще один алгоритм, который отличается например тем, что в нем можно использовать не только евклидово расстояние. Да-да, это алгоритм Partion Around Medoids!

```{r }
# Допустим возьмем матрицу евклидовых расстояний и количество кластеров 2

pm <- pam(x = de, k = 2)
```

## Оптимальное значение кластеров
<br> В данном случае мы попробуем взять некоторые матрицы расстояний и определить для них оптимальное количество кластеров c помощью метода `wss` в функции из пакета `factoextra`. Основная задумка схожа с методом каменистой осыпи. Ось Y будет указывать нам предельную сумму квадратов расстояний между объектами. Берем такое количество кластеров, при котором это значение будет не совсем маленькое (кластеризация будет плохая) и не совсем большое (тогда кластеризация как такова вообще не произойдет)   


```{r }

fviz_nbclust(data, pam, method = "wss")

```

Тода наша визуализация для матриц различных расстояний будет следующая

```{r }
pam_de <- pam(x = de, k = 4)
pam_dm <- pam(x = dm, k = 4)

data_pca <- scale(data, 
                  scale = F) %>% 
  scale(., 
        center = F, 
        scale = apply(., 2, norm, type = "2")) %>% 
  prcomp %$% 
  x[ ,1:2] %>% 
  as.data.frame %>% 
  cbind(., 
        Clusters_1 = factor(pam_de$clustering),
        Clusters_2 = factor(pam_de$clustering))

multiplot(ggplot(data = data_pca, 
                 aes(x = '', 
                     y = pam_de$clustering, 
                     color = pam_de$clustering)) + 
            geom_jitter(color=pam_de$clustering) + 
            ggtitle("PAM algorithm with Euclidien dists"),
          ggplot(data = data_pca, 
                 aes(x ='', 
                     y = pam_dm$clustering, 
                     color = pam_dm$clustering)) + 
            geom_jitter(color=pam_dm$clustering) + 
            ggtitle("PAM algorithm wih Manhatten dists"))




```


## Вывод
Алгоритм Partion Around Medoids также является неплохой альтернативой для кластеризации. Но стоит брать во внимание, что по сравнению с `kmeans()`, данный алгоритм является менее производительным. Главной особенность `PAM` является то, что в качестве расстояний можно использовать не только Евклидово расстояние, но и многие другие (Чебышева, Косинусное, Манхэттанских кварталов и т д) 

# Сравнение различных методов кластеризации

Для разных методов кластеризации у нас получилось различное кол-во кластеров. Так, например, в иерархиеской кластеризации и с помощью алгоритма `kmeans()` оптимальное число кластеров равнялось 3-4, а вот для алгоритма `pam()` оптимальное число кластеров - 4. По сути, при разбиение объектов на кластеры, внутрикластерное расстояние остается приблизительно одинаковое(если смотреть на графики и прикинуть на глаз). Все объекты внутри кластеров распеределяются равномерно и не собираются в какие то скопления. нельз четко сказать, какой из методов кластеризации лучше, потому что все они являются эвристикой 

# Вывод
Набор данных разбивается на 3 кластера. Логично предположить, что данные характеризуют, насколько приятно, хорошо и безопасно проживать в том или ином городе США. Соотвественно, кластеры - это группы городов, в которых “очень хорошо”, “нормально” и “плохо





